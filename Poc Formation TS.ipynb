{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC Power BI TS Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import path\n",
    "import glob\n",
    "import xlsxwriter\n",
    "import openpyxl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Déclaration des fonctions et classes exploitées dans le code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datamanagement:\n",
    "    def __init__(self):\n",
    "        self.source = {} # Contient le nom et la localisation des fichiers sources\n",
    "        self.dim = {} # Contient le nom et les dataframes des tables de dimension\n",
    "        self.fact = {} # Contient le nom et les dataframes liés aux tables de faits\n",
    "        self.changed_keys = {}\n",
    "    \n",
    "    def import_csv(self, filename, engine_val=None, encoding_val='utf-8', sep_val=';',low_memory_val=False):\n",
    "        self.source[filename[:-4]] = pd.read_csv(r'Data/'+ filename, engine=engine_val, encoding=encoding_val, sep=sep_val, low_memory=low_memory_val)\n",
    "        return self.source[filename[:-4]]\n",
    "    \n",
    "    def update_key(self, changed_keys, filename):\n",
    "        self.changed_keys[filename] = changed_keys\n",
    "        self.source[filename].rename(columns=changed_keys , inplace=True)\n",
    "        return self.source[filename]\n",
    "    \n",
    "    def import_dim(self,dimname, dimdataframe):\n",
    "        self.dim[dimname]=dimdataframe\n",
    "        \n",
    "    def import_fact(self,factname, factdataframe):\n",
    "        self.fact[factname] = factdataframe\n",
    "    \n",
    "    def export(self):\n",
    "        for key, value in self.dim.items():\n",
    "            location = \"Transformed data\"\n",
    "            file_name = str(key) + '.csv'\n",
    "            location = os.path.join(location, file_name)  \n",
    "            value.to_csv(location, encoding='utf-8')\n",
    "            \n",
    "        for key, value in self.fact.items():\n",
    "            location = \"Transformed data\"\n",
    "            file_name = str(key) + '.csv'\n",
    "            location = os.path.join(location, file_name)  \n",
    "            value.to_csv(location, encoding='utf-16', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération/Mise à jour du fichier setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data():\n",
    "    all_dir = glob.glob(\"Data/*.csv\")    \n",
    "    all_files_name = [x[5:] for x in all_dir]\n",
    "    all_files_name = [x[:-4] for x in all_files_name]\n",
    "    df_files_col = pd.DataFrame()\n",
    "    p = 1\n",
    "    \n",
    "    for dir in all_dir:\n",
    "        df = pd.read_csv(dir, encoding='utf-8', sep=';')\n",
    "        \n",
    "        for i, col in enumerate(df.columns):\n",
    "            df_files_col.at[p, 'Nom fichier source'] = dir[5:-4]\n",
    "            df_files_col.at[p,'Nom champ source'] = col\n",
    "            df_files_col.at[p,'Type'] = str(df[col].dtypes)\n",
    "            df_files_col.at[p,'Synthèse'] = str(df[col].describe())\n",
    "            df_files_col.at[p, 'Répertoire'] = dir\n",
    "            p = p + i\n",
    "    \n",
    "    df_dir_files = df_files_col.copy()\n",
    "    df_dir_files = df_dir_files[[\"Nom fichier source\", \"Répertoire\"]].drop_duplicates()\n",
    "    \n",
    "    if path.exists(\"Settings & documentation\\Settings.xlsx\"):\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "\n",
    "        writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='xlsxwriter')\n",
    "        workbook  = writer.book\n",
    "        df_dir_files.to_excel(writer, sheet_name='Fichiers Source', index=False)\n",
    "        \n",
    "        df_files_col[['Nom fichier source', 'Nom champ source', 'Type', 'Synthèse' ]].to_excel(writer, sheet_name='Fichiers et colonnes source', index=False)\n",
    "                \n",
    "        worksheet_FCS = writer.sheets[\"Fichiers et colonnes source\"]\n",
    "        worksheet_FS = writer.sheets[\"Fichiers Source\"]\n",
    "        #worksheet1 = workbook.add_worksheet(\"Mapping données\")\n",
    "        \n",
    "        cell_format_FCS = workbook.add_format() \n",
    "        cell_format_FCS.set_text_wrap()\n",
    "        cell_format_FCS.set_align('center')\n",
    "        cell_format_FCS.set_align('vcenter')\n",
    "        \n",
    "        cell_format_FS = workbook.add_format()    \n",
    "        cell_format_FS.set_align('left')\n",
    "                \n",
    "        worksheet_FCS.set_column('A:B', 30, cell_format_FCS)\n",
    "        worksheet_FCS.set_column('C:C', 15, cell_format_FCS)\n",
    "        worksheet_FCS.set_column('D:D', 25, cell_format_FCS)\n",
    "        worksheet_FS.set_column('A:A', 40, cell_format_FS)\n",
    "        worksheet_FS.set_column('B:B', 60, cell_format_FS)\n",
    "\n",
    "        writer.save()\n",
    "        workbook.close()\n",
    "\n",
    "# Fonction créant la feuille Mapping dim dans le fichier Settings\n",
    "# Cette feuille contient l'ensemble des valeurs de dimension et permet de réaliser un mapping pour changer le nom des attributs\n",
    "\n",
    "def add_sheet_mapcol(DicDataframe):\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"Nom dimension\", \"Nom colonne\"],data=[])\n",
    "    \n",
    "    for key, value in DicDataframe.items():        \n",
    "        #if len(key) >= 3:\n",
    "        #    if key[:3] == \"dim\":\n",
    "        for i, col in enumerate(value.columns):\n",
    "            df.at[i, \"Nom dimension\"] = key\n",
    "            df.at[i, \"Nom colonne\"] = col\n",
    "            df.at[i, \"Nouveau nom\"] = \"\"\n",
    "            df.at[i, \"A mapper\"] = \"Non\"                \n",
    "      \n",
    "    workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")  \n",
    "    writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "    writer.book = workbook1\n",
    "    df.to_excel(writer, sheet_name=\"Mapping dim colonne\",engine='openpyxl',index=False)     \n",
    "    writer.save()\n",
    "    writer.close()\n",
    "    \n",
    "    workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")\n",
    "    writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "    writer.book = workbook1\n",
    "    workbook1[\"Mapping dim colonne\"].column_dimensions[\"A\"].width = 20\n",
    "    workbook1[\"Mapping dim colonne\"].column_dimensions[\"B\"].width = 20\n",
    "    workbook1[\"Mapping dim colonne\"].column_dimensions[\"C\"].width = 20\n",
    "    \n",
    "    writer.save()\n",
    "    writer.close()\n",
    "    \n",
    "def update_sheet_mapcol(DicDataframe):\n",
    "    existing_colmap =  pd.read_excel(r\"Settings & documentation\\Settings.xlsx\", sheet_name='Mapping dim colonne') # Contient les valeurs de mapping dim colonne\n",
    "    fresh_col = pd.DataFrame(columns=[\"Nom dimension\", \"Nom colonne\"], data=[]) # Contient l'ensemble des nouvelles valeurs pour Mapping dim colonne\n",
    "    existing_colmap[\"Comparaison\"] = existing_colmap[\"Nom dimension\"] + existing_colmap[\"Nom colonne\"] # Création de la colonne \"Comparaison\" servant à comparer les lignes\n",
    "    to_keep = [] # Contient la liste des valeurs de la colonne \"Comparaison\" à conserver dans existing_colmap\n",
    "    to_add = [] # Contient la liste des valeurs de la colonne \"Comparaison\" à rajouter\n",
    "    p=0\n",
    "    \n",
    "    for key, value in DicDataframe.items():\n",
    "        \n",
    "        for i, col in enumerate(value.columns):\n",
    "            fresh_col.at[p, \"Nom dimension\"] = key\n",
    "            fresh_col.at[p, \"Nom colonne\"] = col\n",
    "            fresh_col.at[p, \"Nouveau nom\"] = \"\"\n",
    "            fresh_col.at[p, \"A mapper\"] = \"Non\"\n",
    "            p+=1\n",
    " \n",
    "    fresh_col[\"Comparaison\"] = fresh_col[\"Nom dimension\"] + fresh_col[\"Nom colonne\"]\n",
    "    to_keep = list(fresh_col[\"Comparaison\"])\n",
    "    existing_colmap = existing_colmap.loc[existing_colmap[\"Comparaison\"].isin(to_keep)]\n",
    "    existing_colmap.reset_index(drop=True, inplace=True)\n",
    "  \n",
    "    # On détermine la liste des nouvelles valeurs absentes de la feuille de mapping\n",
    "    \n",
    "    for val in to_keep:\n",
    "        if val in list(existing_colmap[\"Comparaison\"]):\n",
    "            pass\n",
    "            \n",
    "        else:\n",
    "            to_add.append(val)\n",
    "    \n",
    "    fresh_col = fresh_col.loc[fresh_col[\"Comparaison\"].isin(to_add)]\n",
    "    fresh_col.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    result = existing_colmap.append(fresh_col)\n",
    "    \n",
    "    # On exporte le résultat dans l'onglet Mapping dim colonne de Setting\n",
    "    result = result.drop([\"Comparaison\"], axis=1)\n",
    "    result = result.sort_values([\"Nom dimension\", \"Nom colonne\"]) \n",
    "    export_to_settings(result, \"Mapping dim colonne\",2)\n",
    "    \n",
    "    return result\n",
    "   \n",
    "    \n",
    "import_data()\n",
    "\n",
    "my_data = Datamanagement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction permettant d'exporter un dataframe vers un onglet de setting avec un formatage automatique des colonnes\n",
    "def export_to_settings(dataframe, sheetname, position = None):\n",
    "    letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    numbers = range(0,26,1)\n",
    "    dic_letters = {} # Dictionnaire contenant les lettres de l'alphabet    \n",
    "    val_max = [] # Liste contenant la taille des valeurs les plus longues\n",
    "    \n",
    "    # Création d'un dictionnaire stockant les lettres de l'alphabet et leur position afin d'alimenter les fonctions openpyxl\n",
    "    for i in range(len(numbers)): \n",
    "        dic_letters[numbers[i]] = letters[i]\n",
    "    \n",
    "    # Alimentation de val_max\n",
    "    for col in dataframe.columns:\n",
    "        max = (dataframe[col].astype(str)).str.len().max()        \n",
    "        if max != 0:\n",
    "            val_max.append(int(max)+1)\n",
    "        else:\n",
    "            val_max.append(15)\n",
    "            \n",
    "   # Ouverture du fichier setting et création de l'onglet sheetname\n",
    "\n",
    "    workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")  \n",
    "    writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "    writer.book = workbook1\n",
    "    list_sheet = workbook1.get_sheet_names() # Récupère la liste des feuilles\n",
    "  \n",
    "    if sheetname in list_sheet: # Si l'onglet existe déjà on va le supprimer pour le remplacer\n",
    "        std = workbook1.get_sheet_by_name(sheetname)\n",
    "        workbook1.remove_sheet(std)\n",
    "    \n",
    "    dataframe.to_excel(writer, sheet_name= sheetname, engine='openpyxl', index=False)     \n",
    "    writer.save()\n",
    "    writer.close()\n",
    "    \n",
    "    # Gère la position de la feuille\n",
    "    \n",
    "    workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")  \n",
    "    writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "    writer.book = workbook1\n",
    "    \n",
    "    if position == None:        \n",
    "        pos = len(list_sheet) - 1\n",
    "    else:\n",
    "        pos = position\n",
    "    \n",
    "    sheets = workbook1._sheets\n",
    "        \n",
    "    sheet = sheets.pop(len(list_sheet) - 1)\n",
    "    sheets.insert(pos, sheet)\n",
    "    \n",
    "    writer.save()\n",
    "    writer.close()\n",
    "    \n",
    "    \n",
    "   # Réouverture du fichier setting\n",
    "\n",
    "    workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")  \n",
    "    writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "    writer.book = workbook1\n",
    "    \n",
    "    # Aliemntation du fichier excel avec les données du dataframe\n",
    "    \n",
    "    for i, width in enumerate(val_max):\n",
    "        if width < 11:\n",
    "            workbook1[sheetname].column_dimensions[dic_letters[i]].width = 12\n",
    "        else:\n",
    "            workbook1[sheetname].column_dimensions[dic_letters[i]].width = width\n",
    "            \n",
    "    # Sauvegarde et fermeture du fichier excel\n",
    "    writer.save()\n",
    "    writer.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction mapping des noms de colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction transformant le nom des colonnes suivant le mapping effectué dans l'onglet Mapping dim colonne du fichier setting\n",
    "# Entrée: Dictionnaire key: Nom dataframe  Value : Dataframe\n",
    "# Sortie un dictionnaire \"nom\"/Dataframe avec les valeurs des colonnes actualisées suivant l'onglet de mapping \"Mapping dim colonne\" du fichier settings\n",
    "\n",
    "def map_col(dict_dataframe):\n",
    "    md = pd.read_excel(r\"Settings & documentation\\Settings.xlsx\", sheet_name=\"Mapping dim colonne\")\n",
    "    mdf = md[\"Nom dimension\"].unique()\n",
    "    dic_map = {}\n",
    "    for dim in mdf:\n",
    "        map = md.loc[(md[\"Nom dimension\"] == dim ) & (md[\"Nouveau nom\"].notnull()== True),[\"Nom colonne\",\"Nouveau nom\"]]\n",
    "        map.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "        for i, val in enumerate(map.iterrows()):           \n",
    "            dict_dataframe[dim].rename(columns={map.at[i, \"Nom colonne\"]:map.at[i,\"Nouveau nom\"]},inplace=True)\n",
    "     \n",
    "    return dict_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction générant une liste des valeurs des champs à mapper. Chaque type de valeur sera extraite.\n",
    "# Etape 1: Vérification si l'onglet \"Mapping données\" existe\n",
    "\n",
    "def mapp_data(dict_dataframe):\n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    try:       \n",
    "        to_mapp_data = pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=\"Mapping données\") # Lecture de la feuille Mapping données\n",
    "        to_mapp_data = to_mapp_data.loc[to_mapp_data[\"Valeurs cible\"] != '']\n",
    "        to_mapp_data.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        if to_mapp_data.empty == False:\n",
    "            for i, val in enumerate(to_mapp_data.iterrows()):\n",
    "                dict_dataframe[to_mapp_data.at[i,\"Table\"]][to_mapp_data.at[i,\"Colonnes\"]].replace(to_replace =to_mapp_data.at[i,\"Valeurs actuelles\"], \n",
    "                 value = to_mapp_data.at[i,\"Valeurs cible\"], inplace=True)\n",
    "            \n",
    "    except:\n",
    "            mapp_col = pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=\"Mapping dim colonne\")\n",
    "            mapp_col = mapp_col.loc[mapp_col[\"A mapper\"] == \"Oui\", [\"Nom dimension\", \"Nom colonne\", \"Nouveau nom\"]]\n",
    "            mapp_col[\"Colonne\"] = \"\"\n",
    "            mapp_col.reset_index(inplace=True, drop=True)\n",
    "            mapp_val = pd.DataFrame(columns=[\"Table\", \"Colonnes\", \"Valeurs actuelles\", \"Valeurs cible\"])\n",
    "            p=1\n",
    "            if mapp_col.empty == False:\n",
    "                for i, val in enumerate(mapp_col.iterrows()):\n",
    "                    if str(mapp_col.at[i, \"Nouveau nom\"]) != \"nan\":\n",
    "                        mapp_col.at[i,\"Colonne\"] = mapp_col.at[i, \"Nouveau nom\"]\n",
    "                    else:\n",
    "                         mapp_col.at[i,\"Colonne\"] = mapp_col.at[i, \"Nom colonne\"]\n",
    "\n",
    "                for i,val in enumerate(mapp_col[[\"Nom dimension\", \"Colonne\"]].iterrows()):\n",
    "                    \n",
    "                    if (dict_dataframe[mapp_col.at[i, \"Nom dimension\"]][mapp_col.at[i, \"Colonne\"]]).empty == False:\n",
    "                        list_val = (dict_dataframe[mapp_col.at[i, \"Nom dimension\"]][mapp_col.at[i, \"Colonne\"]]).unique()\n",
    "                        nom_col = mapp_col.at[i, \"Colonne\"]\n",
    "                        nom_table = mapp_col.at[i, \"Nom dimension\"]\n",
    "\n",
    "                    if len(list_val) != 0:\n",
    "                        for i, val in enumerate(list_val):                           \n",
    "                            mapp_val.at[p,\"Table\"] = nom_table\n",
    "                            mapp_val.at[p,\"Colonnes\"] = nom_col\n",
    "                            mapp_val.at[p,\"Valeurs actuelles\"] = val\n",
    "                            mapp_val.at[p,\"Valeurs cible\"] = \"\"\n",
    "                            p += 1\n",
    "                            \n",
    "            if mapp_val.empty:                \n",
    "                pass            \n",
    "            else:\n",
    "                export_to_settings(mapp_val,\"Mapping données\")            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 663 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Ajouter les tables à intégrer ici\n",
    "tcw = my_data.import_csv('191119_TrainingCollectiveWishes.csv') # Demandes collectives non affectées à un plan de formation\n",
    "tiw = my_data.import_csv('191119_TrainingIndividualWishes.csv') # Demandes individuelles non affectées à un plan de formation\n",
    "tp = my_data.import_csv('191119_TrainingPlan.csv') # Plans de formation\n",
    "tpcw = my_data.import_csv('191119_TrainingPlanCollectiveWishes.csv') # Demande de formation collectives prises en charge dans un plan de formation\n",
    "tpiw = my_data.import_csv('191119_TrainingPlanIndividualWishes.csv') # Demande de formation individuelles prises en charge dans un plan de formation\n",
    "tr = my_data.import_csv('191119_TrainingRegister.csv') # Table recensant les inscription aux sessions de formation \n",
    "ts = my_data.import_csv('191119_TrainingSessions.csv') # Table recensant les sessions de formation \n",
    "tsc = my_data.import_csv('191119_TrainingStageCost.csv') # Table contenant les coûts des stages\n",
    "tsv2 = my_data.import_csv('191119_TrainingStagev2.csv') # Table contenant les stages\n",
    "emp = my_data.import_csv('191119_Employees.csv') # Table maître employé\n",
    "empc = my_data.import_csv('191119_EmployementContract.csv') # Table contrat employé\n",
    "indO = my_data.import_csv('191119_IndividualOrganization.csv') # Table organisation employé\n",
    "indpp =my_data.import_csv('191119_IndivPPCsNew.csv') # ?\n",
    "#setting = pd.read_excel(r'Data/Settings.xlsx', sheets=\"Mapp_data\") # Table de paramétrage servant à mapper les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Renommage des clés fonctionnelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ts = my_data.update_key({'clientcode':'SessionID', 'coursecode':'FormationID', 'startdate':'SessionDate'},'191119_TrainingSessions')\n",
    "tsv2 = my_data.update_key({'clientcode':'SessionID'},'191119_TrainingStagev2')\n",
    "tr = my_data.update_key({'clientcode':'SessionID', 'traineusername':'USERNAME'}, '191119_TrainingRegister')\n",
    "tp = my_data.update_key({'plancode':'PlanID'}, '191119_TrainingPlan')\n",
    "emp = my_data.update_key({'username':'USERNAME'}, '191119_Employees')\n",
    "tcw = my_data.update_key({'clientcode':'WishID','coursecode':'FormationID','username':'USERNAME'}, '191119_TrainingCollectiveWishes')\n",
    "tiw = my_data.update_key({'username':'USERNAME', 'employeenumber':'ZY00.MATCLE', 'clientcode':'WishID', 'coursecode':'FormationID'},'191119_TrainingIndividualWishes')\n",
    "tpcw = my_data.update_key({'plan_code':'PlanID', 'wish_code':'WishID'}, '191119_TrainingPlanCollectiveWishes')\n",
    "tpiw = my_data.update_key({'plan_code':'PlanID', 'wish_code':'WishID'} ,'191119_TrainingPlanIndividualWishes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Alimentation des dictionnaires contenant les colonnes et dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "my_dict_dataframe = {\"tcw\": tcw, \"tiw\": tiw, \"tp\" : tp, \"tpcw\" : tpcw, \"tr\" : tr, \"ts\" : ts, \"tsc\" : tsc, \"tsv2\" : tsv2, \"emp\" : emp,\n",
    "           \"empc\" : empc, \"indO\": indO, \"indpp\": indpp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Creation des tables de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de la table dimension demandes de formations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Génération de la table de dimension Souhaits de formation à partir des tables 191119_TrainingCollectiveWishes et 191119_TrainingIndividualWishes \n",
    "dim_wish = tcw.append(tiw,sort=True)\n",
    "dim_wish.reset_index(drop=True, inplace=True)\n",
    "dim_wish[\"Wish_key\"] = dim_wish.index\n",
    "dim_wish.loc[dim_wish.nbtrainees.isna(), \"nbtrainees\"] = 1\n",
    "dim_wish.index.name=\"key_wish\"\n",
    "#dim_wish = df_mapp(dim_wish)\n",
    "my_data.import_dim(\"Souhaits de formation\", dim_wish)\n",
    "\n",
    "#y_data.dim[\"dim_wish_table\"] = dim_wish\n",
    "#my_dict_dataframe[\"dim_wish_table\"]= dim_wish\n",
    "#wish_table.to_excel(r'Transformed data\\wish_table.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de la table dimension plan de formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Génération de la table plan de formation\n",
    "dim_plan = tp\n",
    "val_empty = {\"PlanID\":\"0\", \"nom du plan\" : \"Pas de plan\"} # On insère une ligne pour affecter par la suite les demandes non affectées à une clé plan\n",
    "dim_plan= dim_plan.append(val_empty,ignore_index=True) \n",
    "dim_plan.index.name=\"key_plan\"\n",
    "my_data.import_dim(\"Plan de formation\", dim_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement du mapping des colonnes et des valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated function get_sheet_names (Use wb.sheetnames).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated function get_sheet_by_name (Use wb[sheetname]).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated function remove_sheet (Use wb.remove(worksheet) or del wb[sheetname]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Verification si la colonne Mapping dim existe dans le fichier setting, si non on va la créer\n",
    "try:\n",
    "    pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=\"Mapping dim colonne\")   \n",
    "    Mapp_sheet = True    \n",
    "except:\n",
    "    Mapp_sheet = False\n",
    "    \n",
    "    \n",
    "if Mapp_sheet == True:\n",
    "    update_sheet_mapcol(my_data.dim)\n",
    "    pass\n",
    "else:    \n",
    "    add_sheet_mapcol(my_data.dim)\n",
    "\n",
    "my_data.dim = map_col(my_data.dim)\n",
    "mapp_data(my_data.dim)\n",
    "\n",
    "#print(my_data.dim.keys())\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Création des tables de fait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table de fait souhaits de formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      FormationID                                WishID ZY00.MATCLE  \\\n",
      "0          QHSSE9                               coll012         NaN   \n",
      "1          ACHAC1                               coll009         NaN   \n",
      "2         QHSRG21                               coll030         NaN   \n",
      "3         QHSRG22                               coll006         NaN   \n",
      "4         QHSRG22                               coll033         NaN   \n",
      "...           ...                                   ...         ...   \n",
      "21748      ACHAP1  db0ab039-4d31-481e-9fc7-5a991b67b61c     6902443   \n",
      "21749     QHSRG21  3054ef36-1005-4efb-b1ca-b5d85ed116ca     6902476   \n",
      "21750     QHSRG10  b01eeb7b-4a46-4e73-9fe7-dce8f74ad95f     6902476   \n",
      "21751     QHSRG21                             REG20-108     6902476   \n",
      "21752     QHSSE62                           PROJB20-367     6902520   \n",
      "\n",
      "       employeenumber  nbmen  nbtrainees  nbwomen                   PlanID  \\\n",
      "0            107140.0    NaN          10      NaN        INVIVO GROUP-2017   \n",
      "1            107140.0    NaN          10      NaN        INVIVO GROUP-2017   \n",
      "2            107140.0    NaN          30      NaN              QALIAN-2017   \n",
      "3            107140.0    NaN           5      NaN      GAMM VERT S.A.-2017   \n",
      "4            107140.0    NaN          10      NaN  SEMENCES DE FRANCE-2017   \n",
      "...               ...    ...         ...      ...                      ...   \n",
      "21748             NaN    NaN           1      NaN                      NaN   \n",
      "21749             NaN    NaN           1      NaN      19 - BIOLINE FRANCE   \n",
      "21750             NaN    NaN           1      NaN      19 - BIOLINE FRANCE   \n",
      "21751             NaN    NaN           1      NaN        20_BIOLINE_FRANCE   \n",
      "21752             NaN    NaN           1      NaN                  20_SMAG   \n",
      "\n",
      "              link_date training_system  ...  realsupported_cost wage_cost  \\\n",
      "0      31/10/2016 00:00            PLAN  ...                 NaN       NaN   \n",
      "1      23/11/2016 00:00            PLAN  ...                 NaN       NaN   \n",
      "2      25/11/2016 00:00            PLAN  ...                 NaN       NaN   \n",
      "3      27/10/2016 00:00            PLAN  ...                 NaN       NaN   \n",
      "4      25/11/2016 00:00            PLAN  ...                 NaN       NaN   \n",
      "...                 ...             ...  ...                 ...       ...   \n",
      "21748               NaN             NaN  ...                 NaN       NaN   \n",
      "21749  16/10/2019 15:38             NaN  ...                 NaN       NaN   \n",
      "21750  11/10/2019 15:05             NaN  ...                 NaN       NaN   \n",
      "21751  19/11/2019 09:13             NaN  ...                 NaN       NaN   \n",
      "21752  30/10/2019 16:14             NaN  ...                 NaN       NaN   \n",
      "\n",
      "       external_trainer_wage_cost  incur_cost hardware_cost  \\\n",
      "0                             NaN         NaN           NaN   \n",
      "1                             NaN         NaN           NaN   \n",
      "2                             NaN         NaN           NaN   \n",
      "3                             NaN         NaN           NaN   \n",
      "4                             NaN         NaN           NaN   \n",
      "...                           ...         ...           ...   \n",
      "21748                         NaN         NaN           NaN   \n",
      "21749                         NaN         NaN           NaN   \n",
      "21750                         NaN         NaN           NaN   \n",
      "21751                         NaN         NaN           NaN   \n",
      "21752                         NaN         NaN           NaN   \n",
      "\n",
      "       trainer_transport_cost  trainer_accomodation_cost  \\\n",
      "0                         NaN                        NaN   \n",
      "1                         NaN                        NaN   \n",
      "2                         NaN                        NaN   \n",
      "3                         NaN                        NaN   \n",
      "4                         NaN                        NaN   \n",
      "...                       ...                        ...   \n",
      "21748                     NaN                        NaN   \n",
      "21749                     NaN                        NaN   \n",
      "21750                     NaN                        NaN   \n",
      "21751                     NaN                        NaN   \n",
      "21752                     NaN                        NaN   \n",
      "\n",
      "       trainer_restoration_cost  deliveryteachingaid_cost  roomhire_cost  \n",
      "0                           NaN                       NaN            0.0  \n",
      "1                           NaN                       NaN            0.0  \n",
      "2                           NaN                       NaN            0.0  \n",
      "3                           NaN                       NaN            0.0  \n",
      "4                           NaN                       NaN            0.0  \n",
      "...                         ...                       ...            ...  \n",
      "21748                       NaN                       NaN            NaN  \n",
      "21749                       NaN                       NaN            NaN  \n",
      "21750                       NaN                       NaN            NaN  \n",
      "21751                       NaN                       NaN            NaN  \n",
      "21752                       NaN                       NaN            NaN  \n",
      "\n",
      "[21753 rows x 23 columns]\n",
      "Wall time: 122 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Création des indicateurs de fomation prévisionnelle, ces derniers s'appuient sur les 4 tables liées aux souhaits de formation\n",
    "\n",
    "# Les tables contenant l'ensemble des souhaits de formation 191119_TrainingCollectiveWishes et 191119_TrainingIndividualWishes vont être fusionnées dans la table wish_all\n",
    "# Seules les clés externes et les attributs servant au calcule des indicateurs seront conservés\n",
    "# La table 191119_TrainingIndividualWishes ne contenant pas l'indicateur \"nbtrainees\", nous considérons qu'une ligne équivaut à 1 nbtrainees\n",
    "\n",
    "tcw_staging = tcw.copy()\n",
    "tcw_staging = tcw_staging[[\"WishID\",\"employeenumber\", \"FormationID\", \"nbtrainees\", \"nbmen\", \"nbwomen\"]]\n",
    "tiw_staging = tiw.copy()\n",
    "tiw_staging = tiw_staging[[\"WishID\", \"ZY00.MATCLE\",\"FormationID\"]]\n",
    "tiw_staging[\"nbtrainees\"]=1\n",
    "\n",
    "wish_all = tcw_staging.append(tiw_staging)\n",
    "\n",
    "# Les tables contenant les demandes de formation associées à un plan vont être fusionnées\n",
    "# Etape 1 récupérer les tables 191119_TrainingPlanCollectiveWishes et 191119_TrainingPlanIndividualWishes correspondant aux souhaits de formation associés à des plans\n",
    "# Etape 2 on va supprimer les champs descriptifs: action, default_currency,hourly_wage_rage\n",
    "# Etape 3 on fusionne les tables dans la table wish_plan\n",
    "\n",
    "tpcw_staging = tpcw.copy()\n",
    "tpcw_staging = tpcw_staging.drop(columns=[\"action\", \"default_currency\",\"hourly_wage_rate\"])\n",
    "tpiw_staging = tpiw.copy()\n",
    "tpiw_staging = tpiw_staging.drop(columns=[\"action\", \"default_currency\"])\n",
    "\n",
    "wish_plan = tpcw_staging.copy()\n",
    "wish_plan = wish_plan.append(tpiw_staging)\n",
    "\n",
    "# On fusionne les 2 tables créées afin de créer la table de fait wish_fact\n",
    "\n",
    "wish_fact = pd.merge(wish_all, wish_plan, on = \"WishID\", how='left')\n",
    "wish_fact.reset_index(drop=True, inplace=True)\n",
    "my_data.import_fact(\"Indicateur Prévisonnel\", wish_fact)\n",
    "\n",
    "print(wish_fact)\n",
    "\n",
    "#prev_fact = tpcw[[\"WishID\", \"nombre de stagiaire\"]].copy()\n",
    "\n",
    "#wish_fact = pd.merge(wish_fact, wish_plan[[\"WishID\", \"PlanID\",\"wage_cost\"]], how='left')\n",
    "\n",
    "#wish_fact[\"wage_cost\"]= wish_fact[\"wage_cost\"].astype('float64')\n",
    "\n",
    "#my_data.import_fact()\n",
    "#dim_wish.loc[:, \"Nb_demandes\"]= 1\n",
    "\n",
    "#my_dict_dataframe[\"wish_fact\"] = wish_fact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table de faits des formations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export des données vers le répertoire Transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 978 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#for key, value in my_dict_dataframe.items():\n",
    " #       location = \"Transformed data\"\n",
    "  #      file_name = str(key) + '.csv'\n",
    "   #     location = os.path.join(location, file_name)  \n",
    "    #    value.to_csv(location, encoding='utf16', index=False)\n",
    "    \n",
    "my_data.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
