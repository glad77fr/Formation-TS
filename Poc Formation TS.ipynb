{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC Power BI TS Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import path\n",
    "import glob\n",
    "import xlsxwriter\n",
    "import openpyxl # 3.0\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Déclaration de la classe Datamanagement et des fonctions associées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datamanagement:\n",
    "    def __init__(self):\n",
    "        self.source = {} # Dictionnaire contenant le nom du fichier source et son contenu stocké dans un DataFrame\n",
    "        self.dim = {} # Dictionnaire contenant le nom et les dataframes des tables de dimension\n",
    "        self.fact = {} # Dictionnaire contenant le nom et les dataframes liés aux tables de faits\n",
    "        self.changed_keys = {}        \n",
    "        self.group_col = pd.DataFrame(columns=[\"Nom colonne\", \"Dimension cible\", \"Colonne cible\"], data=[]) # DataFrame contenant les colonnes de regroupement créés\n",
    "        try:\n",
    "            self.settings = pd.ExcelFile(r\"Settings & documentation\\Settings.xlsx\")  # Fichier setting dans un dataframe\n",
    "            self.setting_sheets = self.settings.sheet_names # List des onglets de setting\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def import_csv(self, filename, engine_val=None, encoding_val='utf-8', sep_val=';',low_memory_val=False):\n",
    "        self.source[filename] = pd.read_csv(r'Data/'+ filename + '.csv', engine=engine_val, encoding=encoding_val, sep=sep_val, low_memory=low_memory_val)\n",
    "        return self.source[filename]    \n",
    "    \n",
    "    def update_key(self, changed_keys, filename):\n",
    "        self.changed_keys[filename] = changed_keys\n",
    "        self.source[filename].rename(columns=changed_keys , inplace=True)\n",
    "        return self.source[filename]\n",
    "    \n",
    "    def import_dim(self,dimname, dimdataframe):\n",
    "        self.dim[dimname]=dimdataframe\n",
    "        \n",
    "    def import_fact(self,factname, factdataframe):\n",
    "        self.fact[factname] = factdataframe\n",
    "    \n",
    "    def export(self):\n",
    "        for key, value in self.dim.items():\n",
    "            location = \"Transformed data\"\n",
    "            file_name = str(key) + '.csv'\n",
    "            location = os.path.join(location, file_name)  \n",
    "            value.to_csv(location, encoding='utf-8', index=False)\n",
    "            \n",
    "        for key, value in self.fact.items():\n",
    "            location = \"Transformed data\"\n",
    "            file_name = str(key) + '.csv'\n",
    "            location = os.path.join(location, file_name)  \n",
    "            value.to_csv(location, encoding='utf-16', index=False)\n",
    "            \n",
    "    def import_data(self):\n",
    "# Fonction générant le fichier setting lors du premier chargement si ce dernier n'existe pas\n",
    "# Etape 1 lecture des fichiers présents dans data et extraction des colonnes ainsi que des informations liées à la qualité de données\n",
    "# Etape 2 vérification si le fichier Settings.xlsx existe, s'il existe, il faut éventuellement le modifier avec les nouvelles informations, sinon le créer\n",
    "        \n",
    "        df_files_col = pd.DataFrame()\n",
    "        p = 1\n",
    "\n",
    "        #Etape 1 lecture des fichiers présents dans data et extraction des colonnes ainsi que des informations liées à la qualité de données\n",
    "        for key, value in self.source.items():\n",
    "            df = value\n",
    "\n",
    "            for i, col in enumerate(df.columns):\n",
    "                df_files_col.at[p, 'Nom fichier source'] = key\n",
    "                df_files_col.at[p,'Nom champ source'] = col\n",
    "                df_files_col.at[p,'Type'] = str(df[col].dtypes)\n",
    "                df_files_col.at[p,'Synthèse'] = str(df[col].describe())\n",
    "                df_files_col.at[p, \"Date ajout\"] = str(datetime.date(datetime.now()))\n",
    "                df_files_col.at[p, \"Date modification\"] = str(datetime.date(datetime.now()))\n",
    "                p = p + i\n",
    "\n",
    "        df_dir_files = df_files_col.copy()\n",
    "        df_dir_files = df_dir_files[[\"Nom fichier source\", \"Date ajout\", \"Date modification\"]].drop_duplicates()\n",
    "\n",
    "        if path.exists(\"Settings & documentation\\Settings.xlsx\"):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='xlsxwriter')\n",
    "            workbook  = writer.book\n",
    "            df_dir_files.to_excel(writer, sheet_name='Fichiers Source', index=False)\n",
    "\n",
    "            df_files_col[['Nom fichier source', 'Nom champ source', 'Type', 'Synthèse']].to_excel(writer, sheet_name='Fichiers et colonnes source', index=False)\n",
    "\n",
    "            worksheet_FCS = writer.sheets[\"Fichiers et colonnes source\"]\n",
    "            worksheet_FS = writer.sheets[\"Fichiers Source\"]\n",
    "            #worksheet1 = workbook.add_worksheet(\"Mapping données\")\n",
    "\n",
    "            cell_format_FCS = workbook.add_format() \n",
    "            cell_format_FCS.set_text_wrap()\n",
    "            cell_format_FCS.set_align('center')\n",
    "            cell_format_FCS.set_align('vcenter')\n",
    "\n",
    "            cell_format_FS = workbook.add_format()    \n",
    "            cell_format_FS.set_align('left')\n",
    "\n",
    "            worksheet_FCS.set_column('A:B', 30, cell_format_FCS)\n",
    "            worksheet_FCS.set_column('C:C', 15, cell_format_FCS)\n",
    "            worksheet_FCS.set_column('D:D', 25, cell_format_FCS)\n",
    "            worksheet_FS.set_column('A:A', 40, cell_format_FS)\n",
    "            worksheet_FS.set_column('B:B', 20, cell_format_FS)\n",
    "            worksheet_FS.set_column('C:C', 20, cell_format_FS)\n",
    "            writer.save()\n",
    "            workbook.close()\n",
    "        self.settings = pd.ExcelFile(r\"Settings & documentation\\Settings.xlsx\")  # Fichier setting dans un dataframe\n",
    "        self.setting_sheets = self.settings.sheet_names # List des onglets de setting\n",
    "    def add_sheet_mapcol(self,DicDataframe=None):\n",
    "# Fonction créant la feuille Mapping dim dans le fichier Settings\n",
    "# Cette feuille contient l'ensemble des valeurs de dimension, elle permet de réaliser un mapping pour changer le nom des attributs\n",
    "\n",
    "        if DicDataframe == None:\n",
    "            DicDataframe = self.dim           \n",
    "            \n",
    "        df = pd.DataFrame(columns=[\"Nom dimension\", \"Nom colonne\"],data=[])\n",
    "\n",
    "        for key, value in DicDataframe.items():        \n",
    "\n",
    "            for i, col in enumerate(value.columns):\n",
    "                df.at[i, \"Nom dimension\"] = key\n",
    "                df.at[i, \"Nom colonne\"] = col\n",
    "                df.at[i, \"Nouveau nom\"] = \"\"\n",
    "                df.at[i, \"A mapper\"] = \"Non\"                \n",
    "\n",
    "        workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")  \n",
    "        writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "        writer.book = workbook1\n",
    "        df.to_excel(writer, sheet_name=\"Mapping dim colonne\",engine='openpyxl',index=False)     \n",
    "        writer.save()\n",
    "        writer.close()\n",
    "\n",
    "        workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")\n",
    "        writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "        writer.book = workbook1\n",
    "        workbook1[\"Mapping dim colonne\"].column_dimensions[\"A\"].width = 20\n",
    "        workbook1[\"Mapping dim colonne\"].column_dimensions[\"B\"].width = 20\n",
    "        workbook1[\"Mapping dim colonne\"].column_dimensions[\"C\"].width = 20\n",
    "\n",
    "        writer.save()\n",
    "        writer.close()\n",
    "    \n",
    "    def map_col(self):        \n",
    "# Fonction transformant le nom des colonnes suivant le mapping effectué dans l'onglet Mapping dim colonne du fichier setting\n",
    "# Entrée: Dictionnaire key: Nom dataframe  Value : Dataframe\n",
    "# Sortie un dictionnaire \"nom\"/Dataframe avec les valeurs des colonnes actualisées suivant l'onglet de mapping \"Mapping dim colonne\" du fichier settings\n",
    "\n",
    "        dict_dataframe = self.dim\n",
    "        md = pd.read_excel(r\"Settings & documentation\\Settings.xlsx\", sheet_name=\"Mapping dim colonne\")\n",
    "        mdf = md[\"Nom dimension\"].unique()\n",
    "        dic_map = {}\n",
    "        \n",
    "        for dim in mdf:\n",
    "            map = md.loc[(md[\"Nom dimension\"] == dim ) & (md[\"Nouveau nom\"].notnull()== True),[\"Nom colonne\",\"Nouveau nom\"]]\n",
    "            map.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            for i, val in enumerate(map.iterrows()):           \n",
    "                dict_dataframe[dim].rename(columns={map.at[i, \"Nom colonne\"]:map.at[i,\"Nouveau nom\"]},inplace=True)\n",
    "\n",
    "        my_dim = dict_dataframe    \n",
    "    \n",
    "    def export_to_settings(self, dataframe, sheetname, position = None):\n",
    "# Fonction permettant d'exporter un dataframe vers un onglet de setting avec un formatage automatique des colonnes\n",
    "# Paramètres: \n",
    "# dataframe: correspond au dataframe à importer,\n",
    "# sheetname: correspond au nom de la feuille cible qui sera remplacées,\n",
    "# position: permet de choisir la position de la feuille modifiée dans le fichier excel\n",
    "   \n",
    "        letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "        numbers = range(0,26,1)\n",
    "        dic_letters = {} # Dictionnaire contenant les lettres de l'alphabet    \n",
    "        val_max = [] # Liste contenant la taille des valeurs les plus longues\n",
    "\n",
    "        # Création d'un dictionnaire stockant les lettres de l'alphabet et leur position afin d'alimenter les fonctions openpyxl\n",
    "        for i in range(len(numbers)): \n",
    "            dic_letters[numbers[i]] = letters[i]\n",
    "\n",
    "        # Alimentation de val_max\n",
    "        for col in dataframe.columns:\n",
    "            max = (dataframe[col].astype(str)).str.len().max()        \n",
    "            if max != 0:\n",
    "                val_max.append(int(max)+1)\n",
    "            else:\n",
    "                val_max.append(15)\n",
    "\n",
    "       # Ouverture du fichier setting et création de l'onglet sheetname\n",
    "    \n",
    "        workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")  \n",
    "        writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "        writer.book = workbook1\n",
    "        list_sheet = workbook1.get_sheet_names() # Récupère la liste des feuilles\n",
    "\n",
    "        if sheetname in list_sheet: # Si l'onglet existe déjà on va le supprimer pour le remplacer\n",
    "            std = workbook1.get_sheet_by_name(sheetname)\n",
    "            workbook1.remove_sheet(std)\n",
    "\n",
    "        dataframe.to_excel(writer, sheet_name= sheetname, engine='openpyxl', index=False)     \n",
    "        writer.save()\n",
    "        writer.close()\n",
    "\n",
    "        # Gère la position de la feuille\n",
    "\n",
    "        workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")  \n",
    "        writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "        writer.book = workbook1\n",
    "\n",
    "        if position == None:        \n",
    "            pos = len(list_sheet) - 1\n",
    "        else:\n",
    "            pos = position\n",
    "\n",
    "        sheets = workbook1._sheets\n",
    "\n",
    "        sheet = sheets.pop(len(list_sheet) - 1)\n",
    "        sheets.insert(pos, sheet)\n",
    "\n",
    "        writer.save()\n",
    "        writer.close()\n",
    "\n",
    "       # Réouverture du fichier setting\n",
    "    \n",
    "        workbook1 = openpyxl.load_workbook(r\"Settings & documentation\\Settings.xlsx\")  \n",
    "        writer = pd.ExcelWriter(r\"Settings & documentation\\Settings.xlsx\", engine='openpyxl')\n",
    "        writer.book = workbook1\n",
    "\n",
    "        # Alimentation du fichier excel avec les données du dataframe\n",
    "\n",
    "        for i, width in enumerate(val_max):\n",
    "            if width < 11:\n",
    "                workbook1[sheetname].column_dimensions[dic_letters[i]].width = 12\n",
    "            else:\n",
    "                workbook1[sheetname].column_dimensions[dic_letters[i]].width = width\n",
    "\n",
    "        # Sauvegarde et fermeture du fichier excel\n",
    "        \n",
    "        writer.save()\n",
    "        writer.close()    \n",
    "    \n",
    "    def mapp_data(self):\n",
    "        result = pd.DataFrame()\n",
    "        test=1\n",
    "\n",
    "        try:       \n",
    "            to_mapp_data = pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=\"Mapping données\") # Lecture de la feuille Mapping données\n",
    "            to_mapp_data = to_mapp_data.loc[to_mapp_data[\"Valeurs cible\"].isna() == False]\n",
    "            to_mapp_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            if to_mapp_data.empty == False:\n",
    "                for i, val in enumerate(to_mapp_data.iterrows()):\n",
    "                    self.dim[to_mapp_data.at[i,\"Table\"]][to_mapp_data.at[i,\"Colonnes\"]].replace(to_replace =to_mapp_data.at[i,\"Valeurs actuelles\"], \n",
    "                     value = to_mapp_data.at[i,\"Valeurs cible\"], inplace=True)\n",
    "        except:\n",
    "                mapp_col = pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=\"Mapping dim colonne\")\n",
    "                mapp_col = mapp_col.loc[mapp_col[\"A mapper\"] == \"Oui\", [\"Nom dimension\", \"Nom colonne\", \"Nouveau nom\"]]\n",
    "                mapp_col[\"Colonne\"] = \"\"\n",
    "                mapp_col.reset_index(inplace=True, drop=True)\n",
    "                mapp_val = pd.DataFrame(columns=[\"Table\", \"Colonnes\", \"Valeurs actuelles\", \"Valeurs cible\"])\n",
    "                p=1\n",
    "                if mapp_col.empty == False:\n",
    "                    for i, val in enumerate(mapp_col.iterrows()):\n",
    "                        if str(mapp_col.at[i, \"Nouveau nom\"]) != \"nan\":\n",
    "                            mapp_col.at[i,\"Colonne\"] = mapp_col.at[i, \"Nouveau nom\"]\n",
    "                        else:\n",
    "                             mapp_col.at[i,\"Colonne\"] = mapp_col.at[i, \"Nom colonne\"]\n",
    "\n",
    "                    for i,val in enumerate(mapp_col[[\"Nom dimension\", \"Colonne\"]].iterrows()):\n",
    "\n",
    "                        if (self.dim[mapp_col.at[i, \"Nom dimension\"]][mapp_col.at[i, \"Colonne\"]]).empty == False:\n",
    "                            list_val = (self.dim[mapp_col.at[i, \"Nom dimension\"]][mapp_col.at[i, \"Colonne\"]]).unique()\n",
    "                            nom_col = mapp_col.at[i, \"Colonne\"]\n",
    "                            nom_table = mapp_col.at[i, \"Nom dimension\"]\n",
    "                        \n",
    "                        if len(list_val) != 0:\n",
    "                            for i, val in enumerate(list_val):                           \n",
    "                                mapp_val.at[p,\"Table\"] = nom_table\n",
    "                                mapp_val.at[p,\"Colonnes\"] = nom_col\n",
    "                                mapp_val.at[p,\"Valeurs actuelles\"] = val\n",
    "                                mapp_val.at[p,\"Valeurs cible\"] = \"\"\n",
    "                                p += 1\n",
    "\n",
    "                if mapp_val.empty: \n",
    "                    pass            \n",
    "                else:\n",
    "                    self.export_to_settings(mapp_val,\"Mapping données\")\n",
    "                    \n",
    "    def __compare__(self,sheet_name, cible_dataframe, excl_col= [], position=None):        \n",
    "        source = pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=sheet_name)\n",
    "        print(source)\n",
    "        source[\"compare\"] = \"\"\n",
    "        cible = cible_dataframe\n",
    "        cible[\"compare\"] = \"\"\n",
    "        colonnes_compared= [col for col in list(source.columns) if col not in excl_col]\n",
    "        \n",
    "        for col in colonnes_compared:\n",
    "            source[\"compare\"] = source[\"compare\"] + source[col].astype('str')        \n",
    "        \n",
    "        for col in colonnes_compared:\n",
    "            cible[\"compare\"] = cible[\"compare\"] + cible[col].astype('str')\n",
    "        \n",
    "        to_keep = [line for line in list(source[\"compare\"]) if line in list(cible[\"compare\"])]\n",
    "        \n",
    "        \n",
    "        to_add = [line for line in list(cible[\"compare\"]) if line not in list(source[\"compare\"])]\n",
    "        \n",
    "        result = source.loc[source[\"compare\"].isin(to_keep)]\n",
    "\n",
    "        \n",
    "        if to_add != \"\":\n",
    "            result = result.append(cible.loc[cible[\"compare\"].isin(to_add)])\n",
    "            \n",
    "        result.reset_index(drop=True, inplace=True)\n",
    "        result.drop([\"compare\"], axis=1, inplace=True)\n",
    "        return result\n",
    "        #self.export_to_settings(result,sheet_name,position)\n",
    "    \n",
    "    def update_fichier_source(self):\n",
    "        #Fonction mettant à jour l'onglet \"Fichiers Source\" du fichier Setting\n",
    "        \n",
    "        df_files_col = pd.DataFrame()\n",
    "        p = 1\n",
    "\n",
    "        #lecture des fichiers présents dans data et extraction des noms des fichiers     \n",
    "        for key, value in self.source.items():\n",
    "            \n",
    "            df = value\n",
    "\n",
    "            for i, col in enumerate(df.columns):\n",
    "                df_files_col.at[p, 'Nom fichier source'] = key\n",
    "                df_files_col.at[p,'Nom champ source'] = col\n",
    "                df_files_col.at[p, \"Date ajout\"] = \"\"\n",
    "                df_files_col.at[p, \"Date modification\"] = \"\"\n",
    "                p = p + i\n",
    "\n",
    "        df_dir_files = df_files_col.copy()\n",
    "        df_dir_files = df_dir_files[[\"Nom fichier source\", \"Date ajout\", \"Date modification\"]].drop_duplicates()\n",
    "        result = self.__compare__(\"Fichiers Source\", df_dir_files, [\"Date ajout\", \"Date modification\"],0 )\n",
    "        result[\"Date ajout\"] = result[\"Date ajout\"].fillna(str(datetime.date(datetime.now())))\n",
    "        result[\"Date modification\"] = str(datetime.date(datetime.now()))\n",
    "        \n",
    "        self.export_to_settings(result,\"Fichiers Source\",0)\n",
    "        \n",
    "    def update_columns(self):\n",
    "        #Fonction mettant à jour l'onglet Mapping dim colonne suite à l'ajout des nouvelles dimensions\n",
    "        existing_colmap =  pd.read_excel(r\"Settings & documentation\\Settings.xlsx\", sheet_name='Mapping dim colonne') # Contient les valeurs de mapping dim colonne\n",
    "        fresh_col = pd.DataFrame(columns=[\"Nom dimension\", \"Nom colonne\"], data=[]) # Contient l'ensemble des nouvelles valeurs pour Mapping dim colonne\n",
    "        existing_colmap[\"Comparaison\"] = existing_colmap[\"Nom dimension\"] + existing_colmap[\"Nom colonne\"] # Création de la colonne \"Comparaison\" servant à comparer les lignes\n",
    "        to_keep = [] # Contient la liste des valeurs de la colonne \"Comparaison\" à conserver dans existing_colmap\n",
    "        to_add = [] # Contient la liste des valeurs de la colonne \"Comparaison\" à rajouter\n",
    "        p=0\n",
    "\n",
    "        for key, value in self.dim.items():\n",
    "\n",
    "            for i, col in enumerate(value.columns):\n",
    "                fresh_col.at[p, \"Nom dimension\"] = key\n",
    "                fresh_col.at[p, \"Nom colonne\"] = col\n",
    "                fresh_col.at[p, \"Nouveau nom\"] = \"\"\n",
    "                fresh_col.at[p, \"A mapper\"] = \"Non\"\n",
    "                p+=1\n",
    "\n",
    "        fresh_col[\"Comparaison\"] = fresh_col[\"Nom dimension\"] + fresh_col[\"Nom colonne\"]\n",
    "        to_keep = list(fresh_col[\"Comparaison\"])\n",
    "        existing_colmap = existing_colmap.loc[existing_colmap[\"Comparaison\"].isin(to_keep)]\n",
    "        existing_colmap.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # On détermine la liste des nouvelles valeurs absentes de la feuille de mapping\n",
    "\n",
    "        for val in to_keep:\n",
    "            if val in list(existing_colmap[\"Comparaison\"]):\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                to_add.append(val)\n",
    "\n",
    "        fresh_col = fresh_col.loc[fresh_col[\"Comparaison\"].isin(to_add)]\n",
    "        fresh_col.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        result = existing_colmap.append(fresh_col)\n",
    "\n",
    "        # On exporte le résultat dans l'onglet Mapping dim colonne de Setting\n",
    "        result = result.drop([\"Comparaison\"], axis=1)\n",
    "        result = result.sort_values([\"Nom dimension\", \"Nom colonne\"]) \n",
    "        self.export_to_settings(result, \"Mapping dim colonne\",2)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def update_map_val(self):       \n",
    "        mapp_col = pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=\"Mapping dim colonne\")\n",
    "        mapp_col = mapp_col.loc[mapp_col[\"A mapper\"] == \"Oui\", [\"Nom dimension\", \"Nom colonne\", \"Nouveau nom\"]]\n",
    "        mapp_col[\"Colonne\"] = \"\"\n",
    "        mapp_col.reset_index(inplace=True, drop=True)\n",
    "        mapp_val = pd.DataFrame(columns=[\"Table\", \"Colonnes\", \"Valeurs actuelles\", \"Valeurs cible\"])\n",
    "        p=1\n",
    "        if mapp_col.empty == False:\n",
    "            for i, val in enumerate(mapp_col.iterrows()):\n",
    "                if str(mapp_col.at[i, \"Nouveau nom\"]) != \"nan\":\n",
    "                    mapp_col.at[i,\"Colonne\"] = mapp_col.at[i, \"Nouveau nom\"]\n",
    "                else:\n",
    "                     mapp_col.at[i,\"Colonne\"] = mapp_col.at[i, \"Nom colonne\"]        \n",
    "\n",
    "            for i,val in enumerate(mapp_col[[\"Nom dimension\", \"Colonne\"]].iterrows()):\n",
    "\n",
    "                if (self.dim[mapp_col.at[i, \"Nom dimension\"]][mapp_col.at[i, \"Colonne\"]]).empty == False:\n",
    "                    list_val = (self.dim[mapp_col.at[i, \"Nom dimension\"]][mapp_col.at[i, \"Colonne\"]]).unique()\n",
    "                    nom_col = mapp_col.at[i, \"Colonne\"]\n",
    "                    nom_table = mapp_col.at[i, \"Nom dimension\"]\n",
    "\n",
    "                if len(list_val) != 0:\n",
    "                    for i, val in enumerate(list_val):                           \n",
    "                        mapp_val.at[p,\"Table\"] = nom_table\n",
    "                        mapp_val.at[p,\"Colonnes\"] = nom_col\n",
    "                        mapp_val.at[p,\"Valeurs actuelles\"] = val\n",
    "                        mapp_val.at[p,\"Valeurs cible\"] = \"\"\n",
    "                        p += 1\n",
    "                        \n",
    "        if mapp_val.empty: \n",
    "            pass     \n",
    "        else:\n",
    "            result = self.__compare__(\"Mapping données\", mapp_val,excl_col=[\"Valeurs cible\"])\n",
    "            self.export_to_settings(result,\"Mapping données\",3)\n",
    "            \n",
    "    def add_col_group(self, regroup_name, dim_source, col_source):\n",
    "            \n",
    "        target_sheet = pd.DataFrame(columns=[\"Dimension source\", \"Colonne source\", \"Valeurs sources\", \"Valeurs cibles\"], data=[])\n",
    "        export_name =  \"Gr_\" + dim_source + \"_\" + regroup_name \n",
    "        \n",
    "        # Préparation du dataframe target_sheet\n",
    "        \n",
    "        for i, val in enumerate(list(self.dim[dim_source][col_source].unique())):\n",
    "            target_sheet.at[i, \"Dimension source\"] = dim_source\n",
    "            target_sheet.at[i, \"Colonne source\"] = col_source\n",
    "            target_sheet.at[i, \"Valeurs sources\"] = val\n",
    "        \n",
    "        if export_name in self.setting_sheets:\n",
    "            result = self.__compare__(export_name, target_sheet, excl_col= [\"Valeurs cibles\"])\n",
    "            self.export_to_settings(result,export_name)\n",
    "            replaced_values = list(result.loc[result[\"Valeurs cibles\"].isna() == False, \"Valeurs sources\"])\n",
    "            replacing_values = list(result.loc[result[\"Valeurs cibles\"].isna() == False, \"Valeurs cibles\"])\n",
    "            print(replaced_values)\n",
    "            print(replacing_values)\n",
    "            self.dim[dim_source][export_name] = self.dim[dim_source][col_source]\n",
    "            self.dim[dim_source][export_name].replace(replaced_values, replacing_values, inplace=True)\n",
    "        else:\n",
    "            self.export_to_settings(target_sheet, export_name)\n",
    "                   \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciation de la classe Datamanagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = Datamanagement()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 683 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Ajouter les tables à intégrer ici\n",
    "tcw = my_data.import_csv('191119_TrainingCollectiveWishes') # Demandes collectives non affectées à un plan de formation\n",
    "tiw = my_data.import_csv('191119_TrainingIndividualWishes') # Demandes individuelles non affectées à un plan de formation\n",
    "tp = my_data.import_csv('191119_TrainingPlan') # Plans de formation\n",
    "tpcw = my_data.import_csv('191119_TrainingPlanCollectiveWishes') # Demande de formation collectives prises en charge dans un plan de formation\n",
    "tpiw = my_data.import_csv('191119_TrainingPlanIndividualWishes') # Demande de formation individuelles prises en charge dans un plan de formation\n",
    "tr = my_data.import_csv('191119_TrainingRegister') # Table recensant les inscription aux sessions de formation \n",
    "ts = my_data.import_csv('191119_TrainingSessions') # Table recensant les sessions de formation \n",
    "tsc = my_data.import_csv('191119_TrainingStageCost') # Table contenant les coûts des stages\n",
    "tsv2 = my_data.import_csv('191119_TrainingStagev2') # Table contenant les stages\n",
    "emp = my_data.import_csv('191119_Employees') # Table maître employé\n",
    "empc = my_data.import_csv('191119_EmployementContract') # Table contrat employé\n",
    "indO = my_data.import_csv('191119_IndividualOrganization') # Table organisation employé\n",
    "indpp =my_data.import_csv('191119_IndivPPCsNew') # ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation du fichier setting ou update du fichier setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Nom fichier source  Date ajout Date modification\n",
      "0       191119_TrainingCollectiveWishes  2020-01-14        2020-01-14\n",
      "1       191119_TrainingIndividualWishes  2020-01-14        2020-01-14\n",
      "2                   191119_TrainingPlan  2020-01-14        2020-01-14\n",
      "3   191119_TrainingPlanCollectiveWishes  2020-01-14        2020-01-14\n",
      "4   191119_TrainingPlanIndividualWishes  2020-01-14        2020-01-14\n",
      "5               191119_TrainingRegister  2020-01-14        2020-01-14\n",
      "6               191119_TrainingSessions  2020-01-14        2020-01-14\n",
      "7              191119_TrainingStageCost  2020-01-14        2020-01-14\n",
      "8                191119_TrainingStagev2  2020-01-14        2020-01-14\n",
      "9                      191119_Employees  2020-01-14        2020-01-14\n",
      "10           191119_EmployementContract  2020-01-14        2020-01-14\n",
      "11        191119_IndividualOrganization  2020-01-14        2020-01-14\n",
      "12                  191119_IndivPPCsNew  2020-01-14        2020-01-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:181: DeprecationWarning: Call to deprecated function get_sheet_names (Use wb.sheetnames).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:184: DeprecationWarning: Call to deprecated function get_sheet_by_name (Use wb[sheetname]).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:185: DeprecationWarning: Call to deprecated function remove_sheet (Use wb.remove(worksheet) or del wb[sheetname]).\n"
     ]
    }
   ],
   "source": [
    "my_data.import_data()\n",
    "my_data.update_fichier_source()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Renommage des clés fonctionnelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ts = my_data.update_key({'clientcode':'SessionID', 'coursecode':'FormationID', 'startdate':'SessionDate'},'191119_TrainingSessions')\n",
    "tsv2 = my_data.update_key({'clientcode':'SessionID'},'191119_TrainingStagev2')\n",
    "tr = my_data.update_key({'clientcode':'SessionID', 'traineusername':'USERNAME','trainingplanclientcode':'PlanID', 'trainingwishclientcode':'WishID'}, '191119_TrainingRegister')\n",
    "tp = my_data.update_key({'plancode':'PlanID'}, '191119_TrainingPlan')\n",
    "#emp = my_data.update_key({'username':'USERNAME'}, '191119_Employees')\n",
    "tcw = my_data.update_key({'clientcode':'WishID','coursecode':'FormationID','username':'USERNAME'}, '191119_TrainingCollectiveWishes')\n",
    "tiw = my_data.update_key({'username':'USERNAME', 'employeenumber':'ZY00.MATCLE', 'clientcode':'WishID', 'coursecode':'FormationID'},'191119_TrainingIndividualWishes')\n",
    "tpcw = my_data.update_key({'plan_code':'PlanID', 'wish_code':'WishID'}, '191119_TrainingPlanCollectiveWishes')\n",
    "tpiw = my_data.update_key({'plan_code':'PlanID', 'wish_code':'WishID'} ,'191119_TrainingPlanIndividualWishes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Creation des tables de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de la table dimension demandes de formations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 60 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Génération de la table de dimension Souhaits de formation à partir des tables 191119_TrainingCollectiveWishes et 191119_TrainingIndividualWishes \n",
    "dim_wish = tcw.append(tiw,sort=True)\n",
    "dim_wish.reset_index(drop=True, inplace=True)\n",
    "dim_wish[\"Wish_key\"] = dim_wish.index\n",
    "dim_wish.loc[dim_wish.nbtrainees.isna(), \"nbtrainees\"] = 1\n",
    "dim_wish.index.name=\"key_wish\"\n",
    "my_data.import_dim(\"Souhaits de formation\", dim_wish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de la table dimension plan de formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Génération de la table plan de formation\n",
    "dim_plan = tp\n",
    "\n",
    "# On ajoute une clé technique à la table dim_plan\n",
    "dim_plan[\"key_plan\"] = dim_plan.index\n",
    "\n",
    "# On insère une ligne pour affecter par la suite les demandes non affectées à une clé plan\n",
    "empty_data = {\"PlanID\" : [\"Pas de plan\"], \"key_plan\" : [9999]}\n",
    "empty_plan = pd.DataFrame(data=empty_data)\n",
    "dim_plan= dim_plan.append(empty_plan,ignore_index=True)\n",
    "\n",
    "my_data.import_dim(\"Plan de formation\", dim_plan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement du mapping des colonnes et des valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:181: DeprecationWarning: Call to deprecated function get_sheet_names (Use wb.sheetnames).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:184: DeprecationWarning: Call to deprecated function get_sheet_by_name (Use wb[sheetname]).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:185: DeprecationWarning: Call to deprecated function remove_sheet (Use wb.remove(worksheet) or del wb[sheetname]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Dimension source Colonne source        Valeurs sources  Valeurs cibles\n",
      "0    Plan de formation       planname             18_DEFISOL             NaN\n",
      "1    Plan de formation       planname   20_BIOLINE_CORPORATE             NaN\n",
      "2    Plan de formation       planname  20_UNION_INVIVO_SILOS             NaN\n",
      "3    Plan de formation       planname           20_PHYTEUROP             NaN\n",
      "4    Plan de formation       planname      20_SICA_DE_GOUAIX             NaN\n",
      "..                 ...            ...                    ...             ...\n",
      "127  Plan de formation       planname  18_SEMENCES_DE_FRANCE             NaN\n",
      "128  Plan de formation       planname             18_LOGITIA             NaN\n",
      "129  Plan de formation       planname      18_SICA_DE_GOUAIX             NaN\n",
      "130  Plan de formation       planname       18_LS_PRODUCTION             NaN\n",
      "131  Plan de formation       planname                    NaN             NaN\n",
      "\n",
      "[132 rows x 4 columns]\n",
      "[]\n",
      "[]\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Verification si la colonne Mapping dim existe dans le fichier setting, si non on va la créer\n",
    "try:\n",
    "    pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=\"Mapping dim colonne\")   \n",
    "    Mapp_sheet = True    \n",
    "except:\n",
    "    Mapp_sheet = False\n",
    "    \n",
    "    \n",
    "if Mapp_sheet == True:\n",
    "    my_data.update_columns()\n",
    "    pass\n",
    "else:    \n",
    "    #add_sheet_mapcol(my_data.dim)\n",
    "    my_data.add_sheet_mapcol()\n",
    "\n",
    "my_data.map_col()\n",
    "#my_data.dim = map_col(my_data.dim)\n",
    "\n",
    "my_data.mapp_data()\n",
    "\n",
    "try:\n",
    "    pd.read_excel(r'Settings & documentation\\Settings.xlsx', sheet_name=\"Mapping données\")   \n",
    "    Mapp_val = True    \n",
    "except:\n",
    "    Mapp_val = False\n",
    "    \n",
    "if Mapp_val == True :\n",
    "    my_data.update_map_val()\n",
    "\n",
    "my_data.add_col_group(\"group1\", \"Plan de formation\", \"planname\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Création des tables de fait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table de fait souhaits de formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 176 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Création des indicateurs de fomation prévisionnelle, ces derniers s'appuient sur les 4 tables liées aux souhaits de formation\n",
    "#Grain = 1 ligne correspond à un souhait de formation\n",
    "\n",
    "# Les tables contenant l'ensemble des souhaits de formation 191119_TrainingCollectiveWishes et 191119_TrainingIndividualWishes vont être fusionnées dans la table wish_all\n",
    "# Seules les clés externes et les attributs servant au calcule des indicateurs seront conservés\n",
    "# La table 191119_TrainingIndividualWishes ne contenant pas l'indicateur \"nbtrainees\", nous considérons qu'une ligne équivaut à 1 nbtrainees\n",
    "\n",
    "tcw_staging = tcw.copy()\n",
    "tcw_staging = tcw_staging[[\"WishID\",\"employeenumber\", \"FormationID\", \"nbtrainees\", \"nbmen\", \"nbwomen\"]]\n",
    "tiw_staging = tiw.copy()\n",
    "tiw_staging = tiw_staging[[\"WishID\", \"ZY00.MATCLE\",\"FormationID\"]]\n",
    "tiw_staging[\"nbtrainees\"]=1\n",
    "\n",
    "wish_all = tcw_staging.append(tiw_staging)\n",
    "\n",
    "# Les tables contenant les demandes de formation associées à un plan vont être fusionnées\n",
    "# Etape 1 récupérer les tables 191119_TrainingPlanCollectiveWishes et 191119_TrainingPlanIndividualWishes correspondant aux souhaits de formation associés à des plans\n",
    "# Etape 2 on va supprimer les champs descriptifs: action, default_currency,hourly_wage_rage\n",
    "# Etape 3 on fusionne les tables dans la table wish_plan\n",
    "\n",
    "tpcw_staging = tpcw.copy()\n",
    "tpcw_staging = tpcw_staging.drop(columns=[\"action\", \"default_currency\",\"hourly_wage_rate\", \"training_system\"])\n",
    "tpiw_staging = tpiw.copy()\n",
    "tpiw_staging = tpiw_staging.drop(columns=[\"action\", \"default_currency\", \"training_system\"])\n",
    "\n",
    "wish_plan = tpcw_staging.copy()\n",
    "wish_plan = wish_plan.append(tpiw_staging)\n",
    "\n",
    "# On fusionne les 2 tables créées afin de créer la table de fait wish_fact\n",
    "\n",
    "wish_fact = pd.merge(wish_all, wish_plan, on = \"WishID\", how='left')\n",
    "wish_fact.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# On ajoute la clé technique de la table plan de formation\n",
    "\n",
    "wish_fact = pd.merge(wish_fact, dim_plan[['PlanID','key_plan']], on='PlanID', how='left')\n",
    "\n",
    "# On ajoute l'indicateur in_plan\n",
    "wish_fact[\"In_plan\"] = \"\"\n",
    "wish_fact.loc[wish_fact['key_plan'].isna() == True,\"In_plan\"] = 0\n",
    "wish_fact.loc[wish_fact['key_plan'].isna() == False,\"In_plan\"] = 1\n",
    "\n",
    "# Typage des valeurs\n",
    "wish_fact[\"wage_cost\"] = wish_fact[\"wage_cost\"].str.replace(\",\",\".\", regex=True)\n",
    "\n",
    "wish_fact[\"wage_cost\"] = wish_fact[\"wage_cost\"].astype('float64')\n",
    "wish_fact[\"key_plan\"] = wish_fact[\"key_plan\"].fillna(9999)\n",
    "wish_fact[\"key_plan\"] = wish_fact[\"key_plan\"].astype('int64')\n",
    "\n",
    "\n",
    "# Stockage dans my_data de la table wish_fact \n",
    "\n",
    "my_data.import_fact(\"Indicateurs Prévisonnels\", wish_fact)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table de faits des formations suivi du plan de formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Préparation de la table de fait contenant les indicateurs liés au suivi du plan\n",
    "tr_staging = tr.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export des données vers le répertoire Transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "my_data.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PlanID', 'beginexecutiondate', 'endexecutiondate', 'initialbudget',\n",
      "       'initialnumberofhours', 'key_plan', 'planname', 'state',\n",
      "       'validationdate', 'Gr_Plan de formation_group1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(my_data.dim[\"Plan de formation\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['18_DEFISOL' '20_BIOLINE_CORPORATE' '20_UNION_INVIVO_SILOS'\n",
      " '20_PHYTEUROP' '20_SICA_DE_GOUAIX' '20_SICA_SILO_PORTUAIRE_DE_BORDEAUX'\n",
      " '20_SILO_HUNINGUE' '20_UNION_INVIVO_AGRO' '20_AGRINOVEX'\n",
      " '20_AGROSOLUTIONS' '20_BE_API' '20_BIOLINE_INSURANCE'\n",
      " '20_DEFISOL_SERVICES' '20_SMAG' '20_SEMENCES_DE_FRANCE'\n",
      " '20_BIOLINE_FRANCE' '20_LS_PRODUCTION' '20_INVIVO_DIGITAL_FACTORY'\n",
      " '2020-NEODIS' '2020-INVIVO GROUP' '2020-GAMM VERT' '2020-INVIVO TRADING'\n",
      " '2020-INVIVO EVENTS' '2020- FOOD AND TECH' '2020-OUIFIELD'\n",
      " '2020- GAMM VERT SUD OUEST' \"2020- FRAIS D'ICI\" \"2018_FRAIS D'ICI\"\n",
      " '2018-FOOD & TECH' '2018-QALIAN' '2018-UPSCIENCE' '2018-ADGENE'\n",
      " '2018-SANICOOPA' '2018-EVIALIS FRANCE' '2018-NUTRILAC' '2018-SERMIX'\n",
      " '2018-MERIEL' '2018-NEOVIA' '2018-INVIVO MANAGEMENT' 'TEST' '2019-SERMIX'\n",
      " '2019-NEOVIA' '2019-ADGENE' '2019 - GAMM VERT' '2019-OUIFIELD'\n",
      " \"2019-FRAIS D'ICI\" '2019-NUTRILAC' '2019-NEODIS' '2019-EVIALIS FRANCE'\n",
      " '2019-UPSCIENCE' '2019-SANICOOPA' '2019-GAMM VERT SUD OUEST'\n",
      " '2019-INVIVO EVENTS' '2019- INVIVO TRADING' '2019- FOOD AND TECH'\n",
      " '2019-INVIVO GROUP' '2019 -GAMM VERT OUEST' '19_AGRINOVEX'\n",
      " '19_BIOLINE_CORPORATE' '19_AGROSOLUTIONS' '19_AGROSOLUTIONS_INSURANCE'\n",
      " '19_BE_API' '19_DEFISOL_SERVICES' '19_SICA_PORTUAIRE_DE_BORDEAUX'\n",
      " '19_SILO_HUNINGUE' '19_UNION_INVIVO_AGRO' '19_UNION_INVIVO_SILOS'\n",
      " '19_PHYTEUROP' '19 - SEMENCES DE FRANCE' '19 - BIOLINE FRANCE'\n",
      " '19 - LS PRODUCTION' '19 - SICA DE GOUAIX' '2019 INVIVO MANAGEMENT'\n",
      " 'TEST- 2019 GvSyO' 'Test GVSYC 2019' '19_SMAG' 'ADGENE' 'AGRINOVEX'\n",
      " 'AGROSCIENCES' 'BIOTOP SA' 'EVIALIS France' 'FRAIS D ICI'\n",
      " 'GAMM VERT OUEST' 'GAMM VERT S.A.' 'GAMM VERT SUD-OUEST' 'INVIVO AGRO'\n",
      " 'AGRO SOLUTIONS' 'INVIVO CAMPUS' 'INVIVO GROUP' 'INVIVO LABS'\n",
      " 'INVIVO MANAGEMENT' 'INVIVO NSA SA' 'INVIVO SILOS' 'INVIVO TRADING'\n",
      " 'L.S. PRODUCTION' 'LOGITIA' 'MERIEL' 'NEODIS' 'NUTRILAC' 'OUIFIELD'\n",
      " 'MARCHE COURTAGE' 'PANCOSMA FRANCE' 'QALIAN' 'SANICOOPA'\n",
      " 'SEMENCES DE FRANCE' 'SEPCO' 'SERMIX' 'SICA DE GOUAIX'\n",
      " 'SICA PORTUAIRE DE BORDEAUX' 'HUNINGUE' 'ALISTAR'\n",
      " '18_SICA_PORTUAIRE_DE_BORDEAUX' '18_HUNINGUE' '18_UNION_INVIVO_SILOS'\n",
      " '2018_ GAMM VERT SA' '2018_NEODIS' '2018_INVIVO GROUP'\n",
      " '2018_INVIVO EVENTS' '2018_GAMM VERT OUEST' '2018_GAMM VERT SUD OUEST'\n",
      " '2018_INVIVO TRADING' '18_AGROSCIENCES CORPORATE' '18_AGROSOLUTIONS'\n",
      " '18_UNION_INVIVO_AGRO' '18_BE_API' '18_AGRINOVEX' '18_BIOLINE_FRANCE'\n",
      " '18_SEMENCES_DE_FRANCE' '18_LOGITIA' '18_SICA_DE_GOUAIX'\n",
      " '18_LS_PRODUCTION' nan]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
